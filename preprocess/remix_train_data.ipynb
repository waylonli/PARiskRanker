{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-29T13:21:05.259809Z",
     "start_time": "2024-03-29T13:21:05.254809Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "twenty_train = pd.read_csv('../data/20/train.csv')\n",
    "thirty_train = pd.read_csv('../data/30/train.csv')\n",
    "fifty_train = pd.read_csv('../data/50/train.csv')\n",
    "hundred_train = pd.read_csv('../data/100/train.csv')\n",
    "two_hundred_train = pd.read_csv('../data/200/train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T13:21:10.593681Z",
     "start_time": "2024-03-29T13:21:05.263311Z"
    }
   },
   "id": "c13885380680cd79",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# upon the original training data, add some more groups with the same accountids being in the same group, and make sure there's at least one anomaly in each group\n",
    "for data, group_size in zip([twenty_train, thirty_train, fifty_train, hundred_train, two_hundred_train], [20, 30, 50, 100, 200]):\n",
    "    has_anomalies = data.groupby('accountid')['anomaly'].sum() > 0\n",
    "    has_anomalies_accountids = has_anomalies[has_anomalies].index\n",
    "    new_qids = range(data['qid'].max() + 1, data['qid'].max() + 1 + len(has_anomalies_accountids))\n",
    "    for qid, accountid in zip(new_qids, has_anomalies_accountids):\n",
    "        # create new groups \n",
    "        new_group_df = data[data['accountid'] == accountid].copy()\n",
    "        new_group_df['qid'] = qid\n",
    "        # make sure it does not exceed the original group size, and there's at least one anomaly\n",
    "        while new_group_df['anomaly'].sum() == 0:\n",
    "            new_group_df = new_group_df.sample(min(group_size, len(new_group_df)))\n",
    "        data = pd.concat([data, new_group_df])\n",
    "    os.mkdir(f'../data/{group_size}_extend')\n",
    "    data.to_csv(f'../data/{group_size}_extend/train.csv', index=False)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T13:25:23.932507Z",
     "start_time": "2024-03-29T13:21:10.595680Z"
    }
   },
   "id": "d48e7750924fce4",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T13:25:23.948013Z",
     "start_time": "2024-03-29T13:25:23.935013Z"
    }
   },
   "id": "a5ede839a1c6ab10",
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
